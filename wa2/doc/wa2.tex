\documentclass{article}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{epsf}
\usepackage{color}
% titlepage causes separate title page
% our latex is biased off 1in vertically and horizontally
\newtheorem{theorem}{Theorem}
\setlength{\topmargin}{0.1in}
\setlength{\oddsidemargin}{0in}
\setlength{\evensidemargin}{0in}
\setlength{\headheight}{0in}
\setlength{\headsep}{0in}
\setlength{\textheight}{9in}
\setlength{\textwidth}{6.5in}
% require that floats fill 90% of a page in order for that page to be
% ``float-only''
\renewcommand{\dblfloatpagefraction}{0.9}
\renewcommand{\floatpagefraction}{0.9}
%\renewcommand{\baselinestretch}{1.2} % interline spacing
%\setlength{\parindent}{0in}
%\parskip=10pt plus2pt minus2pt
%\setlength{\unitlength}{0.1in}
%\pagestyle{empty} % no page numbering
\newenvironment{bibparagraph}{\begin{list}{}{ %
    \setlength{\labelsep}{-\leftmargin} %
    \setlength{\labelwidth}{0pt} %
    \setlength{\itemindent}{-\leftmargin} %
    \setlength{\listparindent}{0pt}}}{\end{list}}
\def\makefigure#1#2{\begin{figure}
\begin{center}
\input{#1}
\end{center}
\caption{#2}
\label{#1}
\end{figure}}

\def\limplies{\; \supset \;}
\def\land{\: \wedge \:}
\def\lor{\: \vee \:}
\def\iff{\; \equiv \;}
\def\lnot{\neg}
\def\lforall#1{\forall \: #1 \;}
\def\lexists#1{\exists \: #1 \;}
\def\glitch#1{{\tt #1}} % glitch on
%\def\glitch#1{} % glitch off
\def\comment#1{}
\def\pnil{[\;]}
\def\pif{\; \mbox{\tt :- } \;}
\def\tuple#1{$\langle #1\rangle$}
\def\mtuple#1{\langle #1\rangle}
\def\ceiling#1{\lceil #1\rceil}
\def\floor#1{\lfloor #1\rfloor}
\def\centerps#1{\begin{center}
\leavevmode
\epsfbox{#1}
\end{center}}
\def\argmax{\mathop{\rm argmax}}
\def\argmin{\mathop{\rm argmin}}
\def\grad{\nabla\!}
\def\celsius{^\circ\mbox{C}}
%\long\def\answer#1{}  % comment out for solutions
%\long\def\question#1{#1} % comment out for solutions
\long\def\answer#1{{\color {blue} {\sl #1}}}  % comment in for solution
\long\def\question#1{} % comment in for solution

\def\z{{\bf z}}
\def\x{{\bf x}}
\def\w{{\bf w}}

\newcommand{\Lagr}{\mathcal{L}}
\newcommand\tab[1][1cm]{\hspace*{#1}}

\begin{document}

\begin{titlepage}
    \begin{center}
        \vspace*{4cm}

        \textbf{\Large CS534 - Machine Learning}

        \vspace{0.5cm}
 
        \textbf{\Large Written Homework Assignment 2}
 
        \vspace{1cm}

        Author: Vy Bui

        OSUID: 934370552 \\ 
        
        Email: buivy@oregonstate.edu

        \vfill
             
        \vspace{0.8cm}
      
             
        The School of Electrical Engineering and Computer Science\\
        Oregon State University\\
             
    \end{center}
\end{titlepage}


%Please submit via TEACH electronically.
This assignment covers Perceptron, Kernel methods, SVMs and Naive Bayes
\begin{enumerate}
\item (Subgradient) (2 pts) Consider the $L_1$ form function for 
$\x\in R^d$: $f(\x)=|\x|_1=\sum_{i=1}^d|x_i|$. Show that 
${\bf g}=[g_1, g_2, ...,g_d]^T$ is a subgradient of $f(\x)$ at $\x={\bf 0}$ if 
every $g_i \in [-1,1] $ using the definition of subgradient: $g$ is a subgradient of $f(x)$ at $x_0$ if $\forall x$, $f(x)\geq f(x_0)+g^T(x-x_0)$

\answer{
Let $z = x_0 = 0$ to avoid notation confusion. First, let us consider the right 
side of the inequality $R$. $f(z) = 0$ because $z_i = 0, i \in [1,d]$, hence the 
right side reduces to 

$$
R = \mathbf{0} + \mathbf{g}^T (\mathbf{x - z}) = g_1x_1 + g_2x_2 + ... + g_dx_d
$$

The left side can be calculated as follows
$$f(x) = |x_1| + |x_2| + ... + |x_d|$$

Because $g_i \in [-1,1]$, $g_ix_i \leq |x_i|$ for $i = 1...d$, therefore 
$f(x) \geq f(z) + \mathbf{g}^T (\mathbf{x - z})$, which shows that $g$ is a 
subgradient of $f(x)$ at $x = 0$.
}

\item (Perceptron) (2 pts) Consider the following argument. We know that the 
number of steps for the perceptron algorithm to converge for linearly separable 
data is bounded by $(\frac{D}{\gamma})^2$. If we multiple the input $\x$ by a 
small constant $\alpha$, which effectively reduces the bound on $|\x|$ to 
$D' =\alpha D$, we can reduce the upper bound to $(\alpha \frac{D}{\gamma})^2$. 
Is this argument correct? Why? 

\answer{
This argument is incorrect. Because the scaler $\alpha$ not only affects "the 
radius of data" $D$, but also affects the maximum margin $\gamma$, which is the 
distance between the separating hyperplane to the closest $x_i$. As a result, 
the upper bound of the number of steps becomes $(\frac{\alpha D}{\beta \gamma})^2$, 
where $\beta$ is the effect of scaling $x$ on $\gamma$.
}

\item (Kernel or not). (9 pts) In the following problems, suppose that $K$, 
$K_1$ and $K_2$ are kernels with feature maps $\phi$, $\phi_1$ and $\phi_2$. 
For the following functions $K'(x, z)$, state if they are kernels or not. If 
they are kernels, write down the corresponding $\phi$ in terms of $\phi$, 
$\phi_1$ and $\phi_2$. If they are not kernels, prove that they are not.

\begin{itemize}
\item (2 pts) $K'(\x, \z) = cK(\x, \z)$ for $c > 0$.\\
\answer{This is a valid kernel with the corresponding $\phi' = \sqrt{c}\phi$
}
\item (2 pts) $K'(\x,\z) = cK(\x, \z)$ for $c < 0$.\\
\answer{This is not a valid kernel because $\phi' = \sqrt{c}\phi$ is not defined 
when $c < 0$}.
\item (2 pts) $K'(\x,\z)= c_1K_1(\x, \z)+c_2K_2(\x,\z)$ for $c_1, c_2 >0$.\\
\answer{This is a valid kernel with the corresponding $\phi = \sqrt{c_1}\phi_1 + \sqrt{c_2}\phi_2$}
\item (3 pts) $K'(\x,\z)= K_1(\x, \z)K_2(\x,\z)$ .\\
\answer{This is a valid kernel with the corresponding $\phi = \phi_1\phi_2$
}

\end{itemize}

\newpage
\item (Hard margin SVM) (5 pts) Apply linear SVM without soft margin to the 
following problem.
% \begin{center}
% \begin{figure}[h]
% \includegraphics[width=3in]{svm.pdf}
% \end{figure}
% \end{center}

\begin{itemize} \item[a.] (2pts) Please mark out the support vectors, the 
decision boundary ($w_1x_1+w_2x_2 +b =0$) and $w_1x_1+w_2x_2+b =1$ and 
$w_1x_1+w_2x_2+b =-1$. You don't need to solve the optimization problem for 
this, you should be able to eyeball the solution and find the linear separator 
with the largest margin.

\answer{
As shown in the following figure, the black line $x_1 = 1.5$ is the linear 
separator with the largest margin equal 0.5. The support vectors are the points 
through which the green lines $x_1 = 1$ and $x_1 = 2$ go.

\includegraphics[scale=0.5]{q4a_1.png}

Any change in the slope of the linear separator will decrease the margin, as 
shown in the following figure.

\includegraphics[scale=0.5]{q4a_2.png}
}
\item[b.] (3 pts) Please solve for $w_1, w_2$ and $b$ based on the support 
vectors you identified in (a). Hint: the support vectors would have functional 
margin = 1. \\
\answer{
The functional margin for data point (1.5,0) on the decision boundary, support 
vectors (1,1) and (1,2) can be calculated as follows

$1.5w_1 + b = 0$

$w_1 + w_2 + b = 1$

$2w_1 + w_2 + b = -1$

Solving this linear system results in $w_1 = -2, w_2 = 0, b = 3$.
}
\end{itemize}
\item $L_2$ SVM (10 pts) \\
Given a set of training examples $\{(\x_i, y_i)\}_{i=1}^N$, where 
$y_i\in \{1, -1\}$ for all $i$. The following is the primal formulation of $L_2$ 
SVM, a variant of the standard SVM obtained by squaring the slacks.
\begin{align*}
\min_{\w, b, \xi} \mbox{ }& \frac{1}{2}\w^T\w+c\sum_{i=1}^N \xi_i^2 \\
\mbox{s.t. } & y_i(\w^T\x_i+b)\geq 1-\xi_i \mbox{,   } i \in \{1,\cdots,N\}\\
 & \xi_i\geq 0, \mbox{  }  i\in \{1,\cdots,N\}
\end{align*}

\begin{itemize}
\item [a.] (2pts) Show that removing the second constraint $\xi_i\geq 0$ will 
not change the solution to the problem. In other words, let 
$(\w^*, b^*, {\bf \xi}^*)$ be the optimal solution to the problem without this 
set of constraints, show that  
${\bf \xi}_i^* \geq 0$, $\forall i\in\{1,\cdots,N\} $. 
( Hint: use proof by contradiction by assuming that there exists 
some $\xi_i^*<0$.)\\

\answer{
Assume that there exists some optimal solution $(\mathbf{w}^*,b^*,\mathbf{\xi}^*)$ 
with $\xi_i^*<0, \forall i \in \{1,...,N\}$. Then the solution 
$(\mathbf{w}^*,b^*,\mathbf{0})$ will also satisfy the constraint 
$y_i(\w^T\x_i+b)\geq 1-\xi_i \mbox{}$ and make the objective function smaller. 
This contradicts with the assumption that the solution is optimal, therefore, 
there exists no optimal solution with $\xi_i^*<0, \forall i \in \{1,...,N\}$. 
Hence, removing it from the list of constraints does not change the solution to 
the problem.
}

\item [b.] (2 pts) After removing the second set of constraints, we have a 
simpler problem with only one set of constraints. Now provide the lagrangian of 
this new problem.\\
\answer{
$$
\Lagr(w,b,\alpha,\xi) = \frac{1}{2}\w^T\w+c\sum_{i=1}^N \xi_i^2 - 
\sum_{i=1}^N \alpha_i[y_i(\w^T\x_i + b) - 1 + \xi_i]
$$

s.t. $\alpha_i \geq 0$ for $i = 1,...,m$.
}
\item [c.] (6pts) Derive the dual of this problem. How is it different from the 
standard SVM with hinge loss? Which formulation is more sensitive to outliers?\\
\answer{
Substituting $\w = \sum_{i=1}^N \alpha_i y_i \x_i $ into $\Lagr$ results in

$$
\Lagr(\alpha)
= \frac{1}{2} \sum_{i=1}^N \sum_{j=1}^N (\alpha_i y_i \x_i)^T (\alpha_j y_j \x_j) 
+ \sum_{i=1}^N \frac{\alpha_i}{\xi_i} \xi_i^2
- \sum_{i=1}^N \alpha_i[y_i((\sum_{j=1}^N \alpha_j y_j \x_j)^T\x_i + b) - 1 + \xi_i]
$$

$$
= 
- \frac{1}{2} \sum_{i=1}^N \sum_{j=1}^N \alpha_i \alpha_j y_i y_j \x_i^T \x_j
+ \sum_{i=1}^N \alpha_i \xi_i 
- b \sum_{i=1}^N \alpha_i y_i
+ \sum_{i=1}^N \alpha_i
- \sum_{i=1}^N \alpha_i \xi_i 
$$

$$
= 
\sum_{i=1}^N \alpha_i
- \frac{1}{2} \sum_{i=1}^N \sum_{j=1}^N \alpha_i \alpha_j y_i y_j \x_i^T \x_j
$$

The loss formulation for the dual problem is more sensitive to outliers because 
the degrees of x and y in it are higher than those in the formulation of Hinge 
loss.
}

\end{itemize}
\item (Naive Bayes Classifier) (6 pts) Consider the following training set:
\begin{center}
\begin{tabular}{|c|c|c|c|}\hline
A&B&C&Y\\ \hline
0&1&1&0 \\ \hline
1&1&1&0 \\ \hline
0&0&0&0 \\ \hline
1&1&0&1 \\ \hline
0&1&0&1 \\ \hline
1&0&1&1 \\ \hline
\end{tabular}
\end{center}
\begin{enumerate}
\item (2 pts) Learn a Naive Bayes classifier by estimating
all necessary probabilities (there should be 7 independent probabilities to be 
estimated in total).

\answer{
First, the prior can be calculated as $P(y=1) = 0.5$ and $P(y=0) = 0.5$.

Second, the distribution of A,B,C given y can be computed as

$P(A = 0| y = 1) = 1/3, P(A = 1| y = 1) = 2/3$

$P(B = 0| y = 1) = 1/3, P(B = 1| y = 1) = 2/3$

$P(C = 0| y = 1) = 2/3, P(C = 1| y = 1) = 1/3$

Finally, 

$P(A=1,B=0,C=0) = \sum_j(\prod_{i} P(x_i|y=j) P(y=j)) \newline
= 0.5 \times 1/3 \times 1/3 \times 1/3 + 0.5 \times 2/3 \times 1/3 \times 2/3 
= \frac{5}{54}$
}
\item (3 pts) Compute the probability $P(y=1|A=1, B=0, C=0)$.
\answer{

$P(y=1|A=1, B=0, C=0) = \frac{P(A=1,B=0,C=0|y=1) P(y=1)}{P(A=1,B=0,C=0)}
= \frac{2/3 \times 1/3 \times 2/3 \times 0.5}{5/54} = \frac{4}{5}$
}
\item (1 pts) Suppose we know that the three features A, B and C are independent 
from one another, can we say that the Naive Bayes assumption is valid? (Note 
that the particular data set is irrelevant for this question). If your answer 
is yes, please explain why; if you answer is no please give an counter example.

\answer{
The assumption is not valid because the independence of between A,B, and C is 
not conditional independence, which is required for Naive Bayes assumption. 

Counter example:

Independence: \newline
$P(A=0,B=0,C=0|y=1) = P(A=0)P(B=0)P(C=0) = 1/2 \times 2/6 \times 1/2 = 1/12$

Conditional independence: \newline
$P(A=0,B=0,C=0|y=1) = P(A=0|y=1)P(B=0|y=1)P(C=0|y=1)
= 1/3 \times 1/3 \times 2/3 = 2/27$


}

\end{enumerate}

\item (Naive Bayes learns linear decision boundary.) (6 pts) Consider a naive 
Bayes binary classifier with a set of binary features $x_1, x_2, ...,x_d$. Show 
that the Naive Bayes classifier learns a linear decision boundary 
$w_0+w_1x_1+w_2x_2+...+w_dx_d=0$. Express the weights using the Naive Bayes 
parameters. Hint: consider the decision rule of predicting 
$y=1$ if $P(y=1|\x) > P(y=0|\x)$. This is equivalent to having a decision 
boundary defined by $\log\frac{P(y=1|\x)}{P(y=0|\x)}=0$.

\answer{
The classifier will predict $y=1$ if 
$
P(y=1|\x) \geq P(y=0|\x) \Leftrightarrow \frac{P(\x|y=1)P(y=1)}{P(\x|y=0)P(y=0)} \geq 1 \tab[0.5cm] (1)
$

Applying the naive Bayes assumption, we have 

$$
P(\x|y) = \prod_{i=0}^d P(x_i|y) \tab[0.5cm] (2)
$$

Plugging (2) into (1) results in 

$$
\frac{P(y=1)}{P(y=0)} \prod_{i=0}^d\frac{P(x_i|y=1)}{P(x_i|y=0)} \geq 1 \tab[0.5cm] (3)
$$

To simplify notation, let $P(y=1) = p$, $P(x_i=1|y=1) = k_i$, and $P(x_i=1|y=0) = m_i.$
Because the features are binary, either 0 or 1, we can derive 
$P(x_i|y=1) = k_i^{x_i}(1 - k_i)^{1 - x_i}$ and 
$P(x_i|y=0) = m_i^{x_i}(1 - m_i)^{1 - x_i}$.

Plugging these new notations to (3) we get

$$
\frac{p}{1-p} \prod_{i=1}^d \frac{k_i^{x_i}(1 - k_i)^{1 - x_i}}
{m_i^{x_i}(1 - m_i)^{1 - x_i}} \geq 1
$$

$$
\Leftrightarrow 
(\frac{p}{1-p} \prod_{i=0}^d \frac{1-k_i}{1-m_i}) 
\prod_{i=0}^d (\frac{k_i(1 - m_i)}{m_i (1 - k_i)})^{x_i} \geq 1
$$

$$
\Leftrightarrow
log(\frac{p}{1-p} \prod_{i=0}^d \frac{1-k_i}{1-m_i}) 
+ \sum_{i=0}^d x_i log(\frac{k_i(1 - m_i)}{m_i (1 - k_i)}) \geq 0 \tab[0.5cm] (4)
$$

Because the log term does not contain any $x_i$, it can be seen as a constant c. 
The log inside of the summation is actually $w_i$. Hence, (4) becomes 

$$
c + \sum_{i=0}^d x_i w_i \geq 0
$$

This shows that the classifier learns a linear decision boundary with $w$ equal 
to the log term in the summation in (4).
}
\end{enumerate}

\end{document}
