{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['axes.titlesize'] = 16\n",
    "plt.rcParams[\"legend.edgecolor\"] = 'black'\n",
    "plt.rcParams[\"legend.fontsize\"] = 13\n",
    "\n",
    "train_data = pd.read_csv(\"IA1_train.csv\")\n",
    "val_data = pd.read_csv(\"IA1_dev.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data, column, mu_dict, sigma_dict):\n",
    "    return (data[column] - mu_dict[column])/sigma_dict[column]\n",
    "\n",
    "        \n",
    "def dataProcessing(data, mu_dict, sigma_dict):\n",
    "    data.insert(loc=0, column=\"bias\", value=1)\n",
    "        \n",
    "    # Drop unused features\n",
    "    data = data.drop(columns=[\"id\", \"date\", \"yr_renovated\"])\n",
    "    \n",
    "    for column in data: \n",
    "        if column not in ['bias', 'waterfront', 'price']:\n",
    "            data[column] = normalize(data, column, mu_dict, sigma_dict)\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def dateSplit(data):\n",
    "    data['month'] = [int(x.split(\"/\")[0]) for x in data['date'].values] # month\n",
    "    data['day'] = [int(x.split(\"/\")[1]) for x in data['date'].values] # day\n",
    "    data['year'] = [int(x.split(\"/\")[2]) for x in data['date'].values] # year\n",
    "    return data\n",
    "\n",
    "\n",
    "def reno(data):\n",
    "    data['age_since_renovated'] = data.apply(lambda d: ((d[\"year\"] - d[\"yr_built\"]) if (d[\"yr_renovated\"] == 0) \n",
    "                                                        else (d[\"year\"] - d[\"yr_renovated\"])), axis=1)\n",
    "    return data\n",
    "\n",
    "\n",
    "# Split date\n",
    "train_data = dateSplit(train_data)\n",
    "val_data = dateSplit(val_data)\n",
    "\n",
    "# Age since renovated\n",
    "train_data = reno(train_data)\n",
    "val_data = reno(val_data)\n",
    "\n",
    "# Find mu and sigma for train -> put into dict\n",
    "mu_dict = {}\n",
    "sigma_dict = {}\n",
    "\n",
    "for col in train_data.columns.values:\n",
    "    if col not in ['id', 'date']:\n",
    "        mu_dict[col] = train_data[col].mean()\n",
    "        sigma_dict[col] = train_data[col].std()\n",
    "\n",
    "        \n",
    "# Finish process data\n",
    "train_norm = dataProcessing(train_data, mu_dict, sigma_dict)\n",
    "val_norm = dataProcessing(val_data, mu_dict, sigma_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_norm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_norm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA EXPLORATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Correlation (Report Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "# cant add to script since it uses seaborn\n",
    "\n",
    "corr_matrix = train_norm.corr()\n",
    "plt.figure(figsize=(14,12))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='BuPu')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL TRAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = train_norm.columns.drop(\"price\")\n",
    "n_features = len(features)\n",
    "\n",
    "X_norm = train_norm[features]\n",
    "y = train_norm[\"price\"].to_frame()\n",
    "\n",
    "X_val = val_norm[features]\n",
    "y_val = val_norm[\"price\"].to_frame()\n",
    "\n",
    "# Configuration\n",
    "learning_rates = [1, 0.178, 0.15, 0.1, 0.05, 0.01, 0.001, 0.0001]\n",
    "n_iterations = 4000\n",
    "n = len(X_norm)\n",
    "epsilonLow = 10**(-2) # converge values\n",
    "epsilonHigh = 10**5 # diverge value\n",
    "\n",
    "w_init = np.zeros(n_features).reshape(n_features,1)\n",
    "w = w_init\n",
    "\n",
    "mse_lr = {}\n",
    "w_lr = {}\n",
    "diverge = []\n",
    "\n",
    "for lr in learning_rates: # iterate over learning rates\n",
    "    w = w_init\n",
    "    mse_train = []\n",
    "    div = False\n",
    "    print(\"Learning Rate: \", lr) # DELETE\n",
    "    for iter in range(n_iterations):\n",
    "        # Rename the product to match y label\n",
    "        y_hat = (X_norm.dot(w)).rename(columns={0: \"price\"})\n",
    "\n",
    "        grads = 2/n * X_norm.T.dot(y_hat - y)\n",
    "        w = w - lr * grads\n",
    "\n",
    "        y_pred = (X_norm.dot(w)).rename(columns={0: \"prices\"})\n",
    "        mse = float(((y - y_pred) ** 2).sum() / n)\n",
    "        mse_train.append(mse)\n",
    "\n",
    "        magLw = np.linalg.norm(grads.values) # magnitude of gradient (change in weights)\n",
    "\n",
    "        # Early stop due to tiny change or diverging\n",
    "        if magLw < epsilonLow:\n",
    "            print(\"Converge, iter: \", iter) # DELETE\n",
    "            break\n",
    "\n",
    "        if magLw > epsilonHigh:\n",
    "            print(\"Diverge, iter: \", iter) # DELETE\n",
    "            div = True\n",
    "            break\n",
    "    \n",
    "    diverge.append(div)\n",
    "    mse_lr[str(lr)] = mse_train\n",
    "    w_lr[str(lr)] = w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,8))\n",
    "for i, (lr, mse) in enumerate(mse_lr.items()):\n",
    "    if not diverge[i]: # dont plot diverging val\n",
    "        plt.plot(np.arange(1,len(mse)+1), mse, label=lr, linewidth=3)\n",
    "    \n",
    "plt.legend(title=\"LR\")   \n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.title(\"MSE per Iteration by Learning Rates (Figure 1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report Question:\n",
    "1a) Which learning rate or learning rates did you observe to be good for this particular dataset? What learning rates (if any) make gradient descent diverge?\n",
    "\n",
    "It seems that learning rates approaching $0**{+}$ from 0.178 all were on the path of convergence (but not necessarily reached convergence). A learning rate of 0.178 resulted in the best minimization of the MSE (discussed in part 1b) in 894 iterations. But as values grew smaller, the model continued to converge with a larger MSE and more iterations. This is correct since our steps are becoming smaller and thus learning becomes much slow (would need more iterations to find it gradient value for converge). All learning rates greater than or equal 0.179 caused divergence. A learning rate of 0.179 diverged in 1515 iterations, while a learning rate of 0.178 converged in 894 iterations. As the learning rate increase, the number of iterations needed to diverge becomes smaller (i.e. a learning rate of 0.3 diverged in 11 iterations, 0.5 in 7 iterations, and 1.0 in 4 iterations). The model seemed to have the best convergence (0.178) very close to where it also began to diverge (0.179)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_min = 1000000\n",
    "best_model_norm = \" \"\n",
    "print(\"Figure 2:\\n\")\n",
    "for i, (lr, w) in enumerate(w_lr.items()):\n",
    "    if not diverge[i]:\n",
    "        y_pred = (X_val.dot(w)).rename(columns={0: \"prices\"})\n",
    "        mse = float(((y_val - y_pred) ** 2).sum() / len(y_val))\n",
    "        print(\"Learning Rate:\", lr + \", MSE:\", mse)\n",
    "        if mse < cur_min:\n",
    "            cur_min = mse\n",
    "            best_model_norm = lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report Question \n",
    "1b) Which learning rate leads to the best validation MSE? Between different convergent learning rates, how should we choose one if the validation MSE is nearly identical?\n",
    "\n",
    "Using a learning rate of 0.178 resulted in the lowest MSE (a value of approximately 4.5144). If two different convergent learning rates have nearly the same MSE for the validation data, we should choose the larger learning step. This is because the two learning rates will give the model similar perform, but the larger learning rate will result in a faster training time. For our model, learning rates 0.15 and 0.05 had a difference of 0.0001 in their MSE. However, the model utilizing a learning rate of 0.15 converged early in 882 iterations while the model utlizing 0.05 converged early in 2646 iterations. The extra iterations for the second model ($lr$ = $0.05$) would be unecessary since the MSE difference between them is minimal and not worth the cost trade off of unnecessary training for an extended period of time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop w0, print all weights for best model\\\n",
    "print(\"Best model has a learning rate of: \", best_model_norm, \", with feature importance below:\\n\")\n",
    "feat_importance = w_lr[best_model_norm].rename(columns={\"price\":\"weights\"}).drop(index=\"bias\")\n",
    "for i, idx in enumerate(feat_importance.index.values):\n",
    "    print(idx + \":\", feat_importance.iloc[i].values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report Question\n",
    "1c) What features are the most important in deciding the house prices according to the learned weights?\n",
    "\n",
    "For our model, the most important feature is `waterfront` (indicating whether the apartment was overlooking the waterfront or not) with a weight value of approximately 3.3595. Following this, the `grade` (a scale of building construction design and quality) with a weight value of approximately 1.1139. The third most import feature is `yr_built` (initial house build year) with a weight value of approximately -0.88245. The first two features both increase the house price, while the third decreases the house price."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_norm, y)\n",
    "\n",
    "y_pred = lin_reg.predict(X_val)\n",
    "\n",
    "print(y_pred)\n",
    "len(y_pred)\n",
    "\n",
    "print(\"MSE: %.2f\" % mean_squared_error(y_val, y_pred))\n",
    "print(\"w: \" + str(lin_reg.coef_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Training with non-normalized data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = train_data.columns.drop([\"price\", \"id\", \"date\", \"yr_renovated\"])\n",
    "n_features = len(features)\n",
    "\n",
    "X_non_norm = train_data[features]\n",
    "y = train_data[\"price\"].to_frame()\n",
    "\n",
    "X_val_non_norm = val_data[features]\n",
    "y_val = val_data[\"price\"].to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_non_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL TRAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a)\n",
    "Manual experiments show that the learning rate that yields that best model \n",
    "lies somewhere in the range [10**(-11),10**(-10)). Using binary search to find \n",
    "the best learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(X, y, n_iters: int, lr: float, epsilon_low: float, epsilon_high: float):\n",
    "    converge = True\n",
    "    n = X.shape[0]\n",
    "    n_features = X.shape[1]\n",
    "    w = np.zeros(n_features).reshape(n_features,1)\n",
    "    \n",
    "    mse_train = []\n",
    "    \n",
    "    for iter in range(n_iters):\n",
    "        # Rename the product to match y label\n",
    "        y_hat = (X.dot(w)).rename(columns={0: \"price\"})\n",
    "        \n",
    "        grads = 2/n * X.T.dot(y_hat - y)\n",
    "        w = w - lr * grads\n",
    "        \n",
    "        y_pred = (X.dot(w)).rename(columns={0: \"prices\"})\n",
    "        mse = float(((y - y_pred) ** 2).sum() / n)\n",
    "        mse_train.append(mse)\n",
    "        \n",
    "        magLw = np.linalg.norm(grads.values)\n",
    "        \n",
    "        # Early stop due to tiny change or diverging\n",
    "        if magLw < epsilon_low:\n",
    "            print(\"Learning rate %s plateau at iter #%d, magLW = %f\" % (str(lr), iter, magLw))\n",
    "            break\n",
    "        \n",
    "        if magLw > epsilon_high:\n",
    "            print(\"Learning rate %s Diverge at iter #%d, magLw = %f\" % (str(lr), iter, magLw))\n",
    "            converge = False\n",
    "            break\n",
    "        \n",
    "    return converge, mse_train, w\n",
    "\n",
    "# \n",
    "for lr in [10**(-1),10**(-2),10**(-3), 10**(-4),10**(-5),10**(-6),10**(-7),10**(-8),10**(-9),10**(-10)]:\n",
    "    r_converge, r_mse_train, r_w = fit(X_non_norm, y, n_iters=4000,\n",
    "                                                lr=lr,\n",
    "                                                epsilon_low=10**(-6), epsilon_high=10**10)\n",
    "\n",
    "\n",
    "# Use binary search to search for optimal learning rate\n",
    "mse_lr = {}\n",
    "w_lr = {}\n",
    "diverge = []\n",
    "\n",
    "max_depth = 10\n",
    "l = 10**(-11)\n",
    "r = 10**(-10)\n",
    "cur_depth = 0\n",
    "l_converge, l_mse_train, l_w = fit(X_non_norm, y, n_iters=4000, \n",
    "                                            lr=l, \n",
    "                                            epsilon_low=10**(-6), epsilon_high=10**10)\n",
    "\n",
    "diverge.append(not l_converge)\n",
    "mse_lr[str(l)] = l_mse_train\n",
    "w_lr[str(l)] = l_w\n",
    "\n",
    "r_converge, r_mse_train, r_w = fit(X_non_norm, y, n_iters=4000, \n",
    "                                            lr=r, \n",
    "                                            epsilon_low=10**(-6), epsilon_high=10**10)\n",
    "\n",
    "diverge.append(not r_converge)\n",
    "mse_lr[str(r)] = r_mse_train\n",
    "w_lr[str(r)] = r_w\n",
    "\n",
    "while cur_depth < max_depth:\n",
    "    m = (l + r) / 2\n",
    "    \n",
    "    converge, mse_train, w = fit(X_non_norm, y, n_iters=4000, lr=m, epsilon_low=10**(-6), epsilon_high=10**10)\n",
    "    diverge.append(not converge)\n",
    "    mse_lr[str(m)] = mse_train\n",
    "    w_lr[str(m)] = w\n",
    "    \n",
    "    if l_mse_train[-1] < r_mse_train[-1]:\n",
    "        r = m\n",
    "        r_converge = converge\n",
    "        r_mse_train = mse_train\n",
    "        r_w = w\n",
    "    else:\n",
    "        l = m\n",
    "        l_converge = converge\n",
    "        l_mse_train = mse_train\n",
    "        l_w = w\n",
    "    \n",
    "    cur_depth += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,8))\n",
    "for i, (lr, mse) in enumerate(mse_lr.items()):\n",
    "    if not diverge[i]: # dont plot diverging val\n",
    "        plt.plot(np.arange(1,len(mse)+1), mse, label=lr, linewidth=3)\n",
    "    \n",
    "plt.legend(title=\"LR\")   \n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.title(\"MSE per Iteration by Learning Rates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) The best learning rate and its resulting MSE on the training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On training data\n",
    "cur_min = 1000000\n",
    "best_model_non_norm = \" \"\n",
    "\n",
    "for i, (lr, w) in enumerate(w_lr.items()):\n",
    "    if not diverge[i]:\n",
    "        y_pred = (X_non_norm.dot(w)).rename(columns={0: \"prices\"})\n",
    "        mse = float(((y - y_pred) ** 2).sum() / len(y))\n",
    "        print(\"Learning Rate:\", lr + \", MSE:\", mse)\n",
    "        if mse < cur_min:\n",
    "            cur_min = mse\n",
    "            best_model_non_norm = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On validation data\n",
    "cur_min = 1000000\n",
    "best_model_non_norm = \" \"\n",
    "\n",
    "for i, (lr, w) in enumerate(w_lr.items()):\n",
    "    if not diverge[i]:\n",
    "        y_pred = (X_val_non_norm.dot(w)).rename(columns={0: \"prices\"})\n",
    "        mse = float(((y_val - y_pred) ** 2).sum() / len(y_val))\n",
    "        print(\"Learning Rate:\", lr + \", MSE:\", mse)\n",
    "        if mse < cur_min:\n",
    "            cur_min = mse\n",
    "            best_model_non_norm = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop w0, print all weights for best model\n",
    "print(\"Best model has a learning rate of: \", best_model_non_norm, \", with feature importance below:\\n\")\n",
    "feat_importance = w_lr[best_model_non_norm].rename(columns={\"price\":\"weights\"}).drop(index=\"bias\")\n",
    "for i, idx in enumerate(feat_importance.index.values):\n",
    "    print(idx + \":\", feat_importance.iloc[i].values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Redundancy in features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expectation**:\n",
    "- The importance of sqft_living when using only one feature will be higher than \n",
    "that when both features are used. During the learning, think of the grad with \n",
    "respect to sqft_living15 as a vector whose projection on the grad with respect \n",
    "to sqft_living is non-zero (because they are correlated), we can decompose it \n",
    "to the projection plus another vector. When we use two features, the importance \n",
    "of the \"underlying feature\" (the correlated part of them) is split and carried by two \n",
    "weights. Therefore, when only one feature is used, its weight will carry the \n",
    "entire importance of the \"underlying feature\".\n",
    "- This phenomenon can result in inaccurate interpretation of feature importance \n",
    "if we are not aware of the correlation. (need more reasoning here!)\n",
    "\n",
    "**Questions**: \n",
    "1. Correlation btw the two features is 0.76, which is not really \"high\". What \n",
    "should be the appropriate correlation threshold?\n",
    "2. In industrial settings, should we make sense of the correlations between \n",
    "features or just blindly rely on the stats (hence, machine learning, not human \n",
    "learning). If the reasoning is necessary, what is the justification for this \n",
    "particular correlation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop redundant feature\n",
    "redundant_features = [\"sqft_living15\"]\n",
    "X_norm_wo_r_train = X_norm.drop(redundant_features, axis=1)\n",
    "X_norm_wo_r_val = X_val.drop(redundant_features, axis=1)\n",
    "\n",
    "# Train\n",
    "converge, mse_train, w = fit(X_norm_wo_r_train, y, n_iters=4000, \n",
    "                             lr=0.178, \n",
    "                             epsilon_low=10**(-6), epsilon_high=10**10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_norm_wo_r_pred = (X_norm_wo_r_val.dot(w)).rename(columns={0: \"prices\"})\n",
    "mse = float(((y_val - y_norm_wo_r_pred) ** 2).sum() / len(y_val))\n",
    "\n",
    "print(mse)\n",
    "print(w)\n",
    "print(converge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The importance of \"sqft_living\" when using alone is indeed higher than that when \n",
    "using two features. In particular, w_{sqft_living | non-redundant} = 0.8 and \n",
    "w_{sqft_living | redundant} = 0.77 and w_{sqft_living15 | non-redundant} = 0.14. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "15a1e72ba773099527125ed58d42668c54ab9c98f183bb59c28e4748fcdd480d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
