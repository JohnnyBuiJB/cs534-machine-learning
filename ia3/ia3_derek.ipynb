{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2561dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IA3_skeleton_code import *\n",
    "\n",
    "df_train = pd.read_csv(\"IA3-train.csv\")\n",
    "df_val = pd.read_csv(\"IA3-dev.csv\")\n",
    "\n",
    "tfidfvectorizer = TfidfVectorizer(use_idf=True, lowercase=True)\n",
    "\n",
    "X_train = tfidfvectorizer.fit_transform(df_train['text'])\n",
    "y_train = df_train['sentiment']\n",
    "\n",
    "X_val = tfidfvectorizer.transform(df_val['text'])\n",
    "y_val = df_val['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1040cc42",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "exp = range(-4,5)\n",
    "cVals = list(map(lambda x: 10**(x), exp))\n",
    "\n",
    "acc_train = {}\n",
    "acc_val = {}\n",
    "nSVMs = {}\n",
    "\n",
    "print(\"Training SVM with Quadratic Kernel\")\n",
    "for c in cVals:\n",
    "    print(\"Training on c =\", c)\n",
    "    acc_train[c], acc_val[c], nSVMs[c] = trainSVM(X_train, y_train, X_val, y_val, c, \"poly\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f72fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotAcc(exp, acc_train, acc_val, \"Quadratic SVM\", \"qsvmAcc.jpg\")\n",
    "plotNSV(exp, nSVMs, \"Quadratic SVM\", \"qsvmNSV.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef2af53",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpQTA, tmpQVA, tmpQSVs = optimizeC(X_train, y_train, X_val, y_val, 0.3, 0.7, \"poly\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b0d3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotOptC(tmpQTA, \"Training\", \"optCtrain.jpg\", None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56403036",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotOptC(tmpQVA, \"Validation\", \"optCval.jpg\", (0.5, 0.55), (0.916, 0.918))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c10aac2",
   "metadata": {},
   "source": [
    "### Old results (non-balanced class)\n",
    "It seems that the quadratic kernel with c $\\in$ \\[0.1,1\\] is where the ideal c values falls within. However, values of c below 0.8 does not result in high validation accuracy (below 90%) while reaching high training accuracy (around 99%). If we optimize c based on validation accuracy, this would be c $\\in$ \\[0.9,1\\] but the model is obviously overfittin with near 100% training accuracy (10% difference leads me to think overfitting to training data due to high number of SV's - probably around 6300 support vectors for a model that falls in this c range). Optimal c = 0.95\n",
    "\n",
    "### New results (balanced class)\n",
    "Now, the best range for c (in terms of validation accuracy) was $\\in$ \\[0.3, 0.7\\]. This resulted in extremely high training accuracy (signs of overfitting) but also increased our validation accuracy to a peark of around 0.52 (this is the highest with validation acc = 91.72%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fcd043",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = len(y_train)\n",
    "n_val = len(y_val)\n",
    "\n",
    "balanced_optimalQSVM = SVC(C=0.7, kernel=\"poly\", degree=2, max_iter=25000, class_weight=\"balanced\")\n",
    "unbalanced_optimalQSVM = SVC(C=0.7, kernel=\"poly\", degree=2, max_iter=25000)\n",
    "balanced_optimalQSVM.fit(X_train, y_train)\n",
    "unbalanced_optimalQSVM.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "balanced_y_pred_train = balanced_optimalQSVM.predict(X_train)\n",
    "balanced_y_pred_val = balanced_optimalQSVM.predict(X_val)\n",
    "\n",
    "unbalanced_y_pred_train = unbalanced_optimalQSVM.predict(X_train)\n",
    "unbalanced_y_pred_val = unbalanced_optimalQSVM.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde5d645",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score, accuracy_score, recall_score, cohen_kappa_score\n",
    "from sklearn.metrics import precision_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "balanced_rc = recall_score(y_val, balanced_y_pred_val)\n",
    "balanced_pr = precision_score(y_val, balanced_y_pred_val)\n",
    "balanced_rAcc = accuracy_score(y_val, balanced_y_pred_val)\n",
    "balanced_bAcc = balanced_accuracy_score(y_val, balanced_y_pred_val)\n",
    "balanced_kappa = cohen_kappa_score(y_val, balanced_y_pred_val)\n",
    "\n",
    "unbalanced_rc = recall_score(y_val, unbalanced_y_pred_val)\n",
    "unbalanced_pr = precision_score(y_val, unbalanced_y_pred_val)\n",
    "unbalanced_rAcc = accuracy_score(y_val, unbalanced_y_pred_val)\n",
    "unbalanced_bAcc = balanced_accuracy_score(y_val, unbalanced_y_pred_val)\n",
    "unbalanced_kappa = cohen_kappa_score(y_val, unbalanced_y_pred_val)\n",
    "\n",
    "print(\"Balanced\")\n",
    "print(\"Validation Recall:\", balanced_rc)\n",
    "print(\"Validation Precision:\", balanced_pr)\n",
    "print(\"Validation Accuracy:\", balanced_rAcc)\n",
    "print(\"Balanced Validation Accuracy:\", balanced_bAcc)\n",
    "print(\"Kappa:\", balanced_kappa)\n",
    "\n",
    "print(\"Unbalanced\")\n",
    "\n",
    "print(\"Validation Recall:\", unbalanced_rc)\n",
    "print(\"Validation Precision:\", unbalanced_pr)\n",
    "print(\"Validation Accuracy:\", unbalanced_rAcc)\n",
    "print(\"Balanced Validation Accuracy:\", unbalanced_bAcc)\n",
    "print(\"Kappa:\", unbalanced_kappa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6804f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_val, balanced_y_pred_val))\n",
    "print(classification_report(y_val, unbalanced_y_pred_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70ec17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay(confusion_matrix(y_val, y_pred_val)).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0410a5",
   "metadata": {},
   "source": [
    "### Before using balanced class weights (original results)\n",
    "All metrics for the validation data:\n",
    "- The optimal model accuracy (non-balanced) was 89.88%\n",
    "- The balanced accuracy (accounting for the imbalance of classes) was 77.05%\n",
    "- The accuracy for class 0 (majority class) was 99.14%\n",
    "- The accuracy for class 1 (minorirt class) was 54.96%\n",
    "\n",
    "The model is clearly overfitting to the training data and focusing on the majority class, leaving the minority class to be nearly equivalent to random guessing. We may need to change the way we evaluate and pick models (not sure if we can do that in this report, seems more like a training issue then a evaluation issue - I don't believe changing the value of c will account for class imbalance). Training below will be used with SVC and the \"balanced\" class weight to ensure they are treated evenly.\n",
    "\n",
    "### After using balanced class weights (new results)\n",
    "- The optimal model accuracy (non-balanced) was 91.72%\n",
    "- The balanced accuracy (accounting for the imbalance of classes) was 82.84%\n",
    "- The accuracy for class 0 (majority class) was 98.12%\n",
    "- The accuracy for class 1 (minorirt class) was 67.65%\n",
    "\n",
    "\n",
    "We should probably include some discussion on how we changed from non-balanced to balanced class weights for the SVC model. Assigning class 0 (negative tweets) to all observations in the validation data gave us approximately 89% accuracy, so the original quadratic model was not predicting very good."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ea591a",
   "metadata": {},
   "source": [
    "# Part 3 - SVM with RBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89540b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfvectorizer = TfidfVectorizer(use_idf=True, lowercase=True)\n",
    "\n",
    "X_train = tfidfvectorizer.fit_transform(df_train['text'])\n",
    "y_train = df_train['sentiment']\n",
    "\n",
    "X_val = tfidfvectorizer.transform(df_val['text'])\n",
    "y_val = df_val['sentiment']\n",
    "\n",
    "acc_train = {}\n",
    "acc_val = {}\n",
    "nSVs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e193e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainSVMrbf(X_train, y_train, x_val, y_val, c, g):\n",
    "    n_val = len(y_val)\n",
    "    n_train = len(y_train)\n",
    "    \n",
    "    svm = SVC(C=c, kernel=\"rbf\", gamma=g ,max_iter=25000, class_weight=\"balanced\")\n",
    "    svm.fit(X_train, y_train)\n",
    "\n",
    "    y_pred_train = svm.predict(X_train)\n",
    "    y_pred_val = svm.predict(X_val)\n",
    "\n",
    "    acc_train = (n_train - np.count_nonzero(y_pred_train - y_train)) / n_train\n",
    "    acc_val = (n_val - np.count_nonzero(y_pred_val - y_val)) / n_val\n",
    "    return acc_train, acc_val, svm.n_support_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb73199",
   "metadata": {},
   "outputs": [],
   "source": [
    "gRange = [10**i for i in range(-5,2)]\n",
    "cRange = [10**i for i in range(-4,5)]\n",
    "\n",
    "for c in cRange:\n",
    "    for g in gRange:\n",
    "        print(\"Training on c =\", c, \"| gamma =\", g)\n",
    "        acc_train[(c,g)], acc_val[(c,g)], nSVs[(c,g)] = trainSVMrbf(X_train, y_train, X_val, y_val, c, g)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cc64a4",
   "metadata": {},
   "source": [
    "- Appears that high values of c (>= 10) and high values of gamma (=10) do not converge even with max iterations being 25000 (more than enough for all other combos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883b28a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dictionaries for training and validation accuract to matrix of x=gamma, y=c\n",
    "tAccMat = pd.DataFrame(np.nan, index=cRange, columns=gRange)\n",
    "for i,c in enumerate(cRange):\n",
    "    newRow = [acc_train[(c, g)] for g in gRange]\n",
    "    tAccMat.iloc[i] = newRow\n",
    "    \n",
    "vAccMat = pd.DataFrame(np.nan, index=cRange, columns=gRange)\n",
    "for i,c in enumerate(cRange):\n",
    "    newRow = [acc_val[(c, g)] for g in gRange]\n",
    "    vAccMat.iloc[i] = newRow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10086d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotHeatMap(tAccMat, \"Training\", \"rbfHeatTrain.jpg\")\n",
    "plotHeatMap(vAccMat, \"Validation\", \"rbfHeatVal.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f91bceb",
   "metadata": {},
   "source": [
    "#### Q1 - Optimize C & Gamma\n",
    "- Optimizing based on accuracy from above heatmap (validation).\n",
    "- General trend of validation accuracy .8952 and .9128 corresponding to training accuracy before the model overfits. I chose a gamma and c in these ranges to optimize before model starts overfitting.\n",
    "- Best combination in form of (c, gamma) from above: (10, 0.001), (10, 0.01)\n",
    "    - Optimizing in this range: c $\\in [9,11]$ and gamma $\\in [0.005, 0.04]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238b92c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_tAcc = {}\n",
    "opt_vAcc = {}\n",
    "\n",
    "gRange = np.arange(0.005, 0.041, 0.005)\n",
    "cRange = np.arange(9, 11.1, 0.25)\n",
    "\n",
    "for c in cRange:\n",
    "    for g in gRange:\n",
    "        g = round(g, 3) # fix floating point error\n",
    "        print(\"Training on c =\", c, \"| gamma =\", g)\n",
    "        opt_tAcc[(c,g)], opt_vAcc[(c,g)], _ = trainSVMrbf(X_train, y_train, X_val, y_val, c, g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947e6230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dictionaries for training and validation accuract to matrix of x=gamma, y=c\n",
    "gColNames = [round(g,3) for g in gRange]\n",
    "tAccMat = pd.DataFrame(np.nan, index=cRange, columns=gColNames)\n",
    "for i,c in enumerate(cRange):\n",
    "    newRow = [opt_tAcc[(c, round(g,3))] for g in gRange]\n",
    "    tAccMat.iloc[i] = newRow\n",
    "    \n",
    "vAccMat = pd.DataFrame(np.nan, index=cRange, columns=gColNames)\n",
    "for i,c in enumerate(cRange):\n",
    "    newRow = [opt_vAcc[(c, round(g,3))] for g in gRange]\n",
    "    vAccMat.iloc[i] = newRow\n",
    "    \n",
    "plotHeatMap(tAccMat, \"Training\", \"rbfHeatOptTrain.jpg\")\n",
    "plotHeatMap(vAccMat, \"Validation\", \"rbfHeatOptVal.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a060afa",
   "metadata": {},
   "source": [
    "The best model (without overfitting) uses the parameters $c=10$, $gamma=0.02$. The training accuracy is 95.33% and the validation accuracy is 92.12% (some overfitting, but very minimal). Even as the model begins to overfit (last 2 columns), the validation accuracy does not increase much past the optimal model (about +0.04% when training accuracy increases by +4.92%)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5004964",
   "metadata": {},
   "source": [
    "#### Q2 - Accuracy with fixed gamma and across c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4367f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No code needed (use original heatmap looking across each column - fixed gamma - top->down)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e493665c",
   "metadata": {},
   "source": [
    "#### Q3 - - Accuracy with fixed c and across gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bce806b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No code needed (use original heatmap looking across each row - fixed c - left->right)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc2378f",
   "metadata": {},
   "source": [
    "#### Q4 - plot support vectors across fixed gamma = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d79620",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixedG = [sum(nSVs[(c, 0.1)]) for c in cRange]\n",
    "SVplot(range(-4,5), fixedG, \"C (Gamma = 0.1)\", \"cSVcount.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7808b3dd",
   "metadata": {},
   "source": [
    "#### Q5 - plot support vectors across fixed c=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fa0e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixedC = [sum(nSVs[(10, g)]) for g in gRange]\n",
    "SVplot(range(-5,2), fixedC, \"Gamma (C=10)\", \"gammaSVcount.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4d0c83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "15a1e72ba773099527125ed58d42668c54ab9c98f183bb59c28e4748fcdd480d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
