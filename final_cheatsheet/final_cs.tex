
\documentclass[5pt]{article}
\usepackage[landscape]{geometry}
\usepackage{url}
\usepackage{multicol}
\usepackage{amsmath}
\usepackage{esint}
\usepackage{amsfonts}
\usepackage{tikz}
\usetikzlibrary{decorations.pathmorphing}
\usepackage{amsmath,amssymb}

\usepackage{colortbl}
\usepackage{xcolor}
\usepackage{mathtools}
\usepackage{amsmath,amssymb}
\usepackage{enumitem}

\usepackage{epsf}
\usepackage{float}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\makeatletter

\def\argmax{\mathop{\rm argmax}}
\def\argmin{\mathop{\rm argmin}}
\def\argave{\mathop{\rm argave}}

\newcommand*\bigcdot{\mathpalette\bigcdot@{.5}}
\newcommand*\bigcdot@[2]{\mathbin{\vcenter{\hbox{\scalebox{#2}{$\m@th#1\bullet$}}}}}
\makeatother

\title{130 Cheat Sheet}
\usepackage[brazilian]{babel}
\usepackage[utf8]{inputenc}

\advance\topmargin-.8in
\advance\textheight3in
\advance\textwidth3in
\advance\oddsidemargin-1.5in
\advance\evensidemargin-1.5in
\parindent0pt
\parskip2pt
\newcommand{\hr}{\centerline{\rule{3.5in}{1pt}}}
%\colorbox[HTML]{e4e4e4}{\makebox[\textwidth-2\fboxsep][l]{texto}
\begin{document}

\begin{multicols*}{2}

\tikzstyle{mybox} = [draw=black, fill=white, very thick,
    rectangle, rounded corners, inner sep=10pt, inner ysep=10pt]
\tikzstyle{fancytitle} =[fill=black, text=white, font=\bfseries]

\newpage
%------------ General ML ---------------
\begin{tikzpicture}
\node [mybox] (box){%
    \begin{minipage}{0.47\textwidth}
    \end{minipage}
};
%------------ General ML Header ---------------------
\node[fancytitle, right=10pt] at (box.north west) {General ML};

\end{tikzpicture}

%------------ Decision Tree ---------------
\begin{tikzpicture}
\node [mybox] (box){%
    \begin{minipage}{0.47\textwidth}
    \end{minipage}
};
%------------ Decision Tree Header ---------------------
\node[fancytitle, right=10pt] at (box.north west) {Decision Tree};

\end{tikzpicture}


%------------ Emsemble Methods ---------------
\begin{tikzpicture}
\node [mybox] (box){%
    \begin{minipage}{0.47\textwidth}
    \end{minipage}
};
%------------ Emsemble Methods Header ---------------------
\node[fancytitle, right=10pt] at (box.north west) {Emsemble Methods};

\end{tikzpicture}

%------------ Clustering ---------------
\begin{tikzpicture}
\node [mybox] (box){%
    \begin{minipage}{0.47\textwidth}
        
        
        \textbf{Hierarchical Clustering}: there are 2 approaches, bottom up (agglomerative) and top down (divisive)

        \textbf{Hierarchical Agglomerative Clustering}: starts with each sample in a separate cluster, then merge the pair of clusters that have the smallest "distance" until there is only one cluster left.

        (1) Single Link: distance between the nearest pair of points across two clusters
        
        (2) Complete Link: distance between the furthest pair of points across two clusters

        (3) Average link: average distance of all cross cluster pairs

        (4) Centroid: distance between the means of the two clusters
        
        \textbf{Single Link vs. Complete Link} (1) Single link can produce straggling clusters (chains of clusters) whereas complete link creates clusters with roughly equal size $\rightarrow$ creates more useful organization of data (2) complete link might pay too much attention to outliers
        
        \textbf{Interpret Dendrogram}: (1) a horizontal cut creates a partition (2) large gaps indicate good cutting points

        \textbf{Flat Clustering}: K-means and Mixture of Gaussian

        \textbf{Expectation Maximization}
    \end{minipage}
};
%------------ Clustering Header ---------------------
\node[fancytitle, right=10pt] at (box.north west) {Clustering};

\end{tikzpicture}

%------------ K-means ---------------
\begin{tikzpicture}
\node [mybox] (box){%
    \begin{minipage}{0.47\textwidth}
        Assume that all clusters have the same shape (same distribution)???

        \textbf{}

        \textbf{Objective}: minimize the distance between the points and the centroid

        \textbf{Algorithm}:
        
        (1) Randomly pick k centroids

        (2) Assign the samples to the k clusters

        (3) Update the centroids to reduce the cost (inertia), if there is at least a centroid update, go back to step (2)

        \textbf{Metrics}: (1) inertia: mean squared distance between each instance and its closest centroid.

        \textbf{}
    \end{minipage}
};
%------------ K-means Header ---------------------
\node[fancytitle, right=10pt] at (box.north west) {K-means};

\end{tikzpicture}


%------------ Mixture of Gaussian ---------------
\begin{tikzpicture}
\node [mybox] (box){%
    \begin{minipage}{0.47\textwidth}
    \end{minipage}
};
%------------ Mixture of Gaussian Header ---------------------
\node[fancytitle, right=10pt] at (box.north west) {Mixture of Gaussian};

\end{tikzpicture}


%------------ Dimensionality Reduction ---------------
\begin{tikzpicture}
\node [mybox] (box){%
    \begin{minipage}{0.47\textwidth}
    \textbf{Types}:
    (1) \textit{Projection}: Principal Components (PCA), and (2) \textit{Manifold Learning}: t-Distributed Stochastic Neighbor Embedding (t-SNE) and ISOMAP
    

    
    \end{minipage}
};
%------------ Dimensionality Reduction Header ---------------------
\node[fancytitle, right=10pt] at (box.north west) {Dimensionality Reduction};
\end{tikzpicture}

%------------ PCA ---------------
\begin{tikzpicture}
\node [mybox] (box){%
    \begin{minipage}{0.47\textwidth}
    \textbf{Idea}: (1) identifies the hyperplane that lies closest to the data, (2) \underline{projects} the data onto it.

    \textbf{Objective}: (1) maximizes the amount of variance (most likely lose less information) or (2) minimizes the mean squared distance between the original dataset and its projection

    \textbf{Algorithm}

    \textbf{Notes}: (1) PCA assumes the dataset is centered around the origin, (2) choose the number of dimensions that add up to at least 95\% of the variance of the original dataset.
    \end{minipage}
};
%------------ PCA Header ---------------------
\node[fancytitle, right=10pt] at (box.north west) {PCA};
\end{tikzpicture}

%------------ ISOMAP ---------------
\begin{tikzpicture}
\node [mybox] (box){%
    \begin{minipage}{0.47\textwidth}
    \end{minipage}
};
%------------ ISOMAP Header ---------------------
\node[fancytitle, right=10pt] at (box.north west) {ISOMAP};
\end{tikzpicture}

%------------ t-SNE ---------------
\begin{tikzpicture}
\node [mybox] (box){%
    \begin{minipage}{0.47\textwidth}
        \textbf{Idea}: converts similarities between data points to joint probabilities and tries to minimize the Kullback-Leibler divergence between the joint probabilities of the low-dimensional embedding and the high-dimensional data.

        \textbf{Properties}: (1) preserve local structure (2) 

        \textbf{Notes}: (1) has non-convex cost function, hence different initializations might result in different results ()
        
        \textbf{Optimization}: There are 5 parameters that control the optimization of t-SNE: 
        
        (1) perplexity: $k = 2^{(S)}$ where $S$ is the Shannon entropy of the conditional probability distribution. $k$ is the number of nearest neighbors t-SNE considers when generating the conditional probabilities. Larger $k \rightarrow $ more nearest neighbors $\rightarrow$ less sensitive to small structures.
        
        (2) early exaggeration factor (3) learning rate (4) maximum number of iterations (5) angle

        \textbf{Pros}
        
        \textbf{Cons}: (1) computationally expensive (2) stochastic, therefore different initializations yield different results (3) global structure it not explicitly preserved (4) can't be applied to new data points
    \end{minipage}
};
%------------ t-SNE Header ---------------------
\node[fancytitle, right=10pt] at (box.north west) {t-SNE};
\end{tikzpicture}


\newpage
%------------ Neural Network ---------------
\begin{tikzpicture}
\node [mybox] (box){%
    \begin{minipage}{0.47\textwidth}
        \textbf{Activation Functions}: (1) Sigmoid function $\sigma(x) = \frac{1}{1 + e^{-x}}$, (2) Tanh function $tanh(x) = 2\sigma(2x) - 1$, (3) Rectified Linear Unit (ReLu) function $f(x) = max(0,x)$.

        \textbf{Representation Power}: 

        \textbf{Universial Function Approximator} (optional)

        \textbf{Backpropagation Training}: (1) aims to find weights that minimize some loss function (2) applies chain rule for gradient, starting from output back to the first layer.

        $\frac{d\sigma(x)}{dx} = \sigma(x) (1 - \sigma(x))$

        Correction signal delta to weight $k$ for sample $i$: $\frac{\partial J^i}{\partial W_k} = \delta_k^i A^i$, where $\delta_k^i = (\hat{y}^i - y^i)\hat{y}^i(1 - \hat{y}^i)$ and $A$ is the output of the hidden layer.

        \textcolor{red}{Work out Backpropagation with a better and more generalized notation?}

        \textcolor{red}{???}
        \textbf{Impact of ReLu}: (1) reduces the issue of \textcolor{red}{vanishing gradient}, (2) introduces sparsity in the hidden layer activations, which makes the model less prone to overfitting, (3) 

        \textbf{Training}:

        (1) \underline{Batch} learning sums up the gradient for all of examples and take a combined gradient step

        (2) \underline{Online} learning takes a gradient step for each example

        (3) \underline{Momentum} learning combines the current gradient with the previous update direction to ensure smoother convergence

        \textbf{Remarks on Training}: 
        
        (1) not guarantee to converge, may oscillate or reach local minima $\rightarrow$ try different initializations (2) requires lots of data
        (3) might be overtrained, requires early stopping using holdout validation set to detect generalization error degradation (4) two few hidden units $\rightarrow$ underfitting, too many $\rightarrow$ overfitting. Cross-validation can be used to search for a good number. \underline{Weight decay} can help mitigate overfitting, similar to reguralization. (5) appropriate input/output encoding is crucial (6) use a good activation function is important

        \textbf{Proper initialization}: (1) start with simple models by keeping all weights near zero (2) and different from one another to learn from different "angles".
    \end{minipage}
};
%------------ Neural Network Header ---------------------
\node[fancytitle, right=10pt] at (box.north west) {Neural Network};
\end{tikzpicture}

\end{multicols*}
\end{document}