{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IA2_skeleton_code import normalize\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_train = pd.read_csv(\"IA2-train.csv\")\n",
    "df_val = pd.read_csv(\"IA2-dev.csv\")\n",
    "\n",
    "# Preprocess\n",
    "numerical_feas = [\"Age\", \"Annual_Premium\", \"Vintage\"]\n",
    "\n",
    "df_norm_train = pd.DataFrame(df_train)\n",
    "df_norm_val = pd.DataFrame(df_val)\n",
    "\n",
    "for col in numerical_feas:\n",
    "    mu = df_train[col].mean()\n",
    "    sigma = df_train[col].std()\n",
    "\n",
    "    df_norm_train[col] = normalize(df_train, col, mu, sigma)\n",
    "    df_norm_val[col] = normalize(df_val, col, mu, sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IA2_skeleton_code import sigmoid\n",
    "\n",
    "def LR_L2_train(train_data, val_data, _lambda, alpha, isNoisy):\n",
    "    features = train_data.columns.drop(\"Response\")\n",
    "    X_train = train_data[features]\n",
    "    y_train = train_data[\"Response\"].to_frame()\n",
    "    \n",
    "    X_val = val_data[features]\n",
    "    y_val = val_data[\"Response\"].to_frame()\n",
    "    \n",
    "    y_train = y_train.rename(columns={\"Response\": 0})\n",
    "    y_val = y_val.rename(columns={\"Response\": 0})\n",
    "    \n",
    "    n = len(X_train)\n",
    "    n_features = len(X_train.columns.values)\n",
    "    \n",
    "    # Found through experimenting (might need to be changed for large lambda?)\n",
    "    if isNoisy:\n",
    "        epsilon = 0.0000000002\n",
    "    else:\n",
    "        epsilon = 0.0000000001\n",
    "            \n",
    "    # TODO: randomized?\n",
    "    w = (np.ones(n_features) * 0.2).reshape(n_features,1)\n",
    "    acc_train = []\n",
    "    acc_val = []\n",
    "    grad_vals = []\n",
    "    \n",
    "    converged = False\n",
    "    iter = 1\n",
    "    while not converged:\n",
    "        log_odds = sigmoid(X_train.dot(w))\n",
    "        grad = ((y_train - log_odds).T.dot(X_train)).T / n\n",
    "        w += alpha * grad\n",
    "        \n",
    "        # Extra info\n",
    "        # log_loss_pos = y_train.T.dot(np.log(log_odds))[0][0]\n",
    "        # log_loss_neg = (1 - y_train.T).dot(1 - np.log(log_odds))[0][0]\n",
    "        # log_loss_avg = -(log_loss_pos + log_loss_neg) / n\n",
    "        # reg = _lambda * np.linalg.norm(w) ** 2\n",
    "        # log_loss = log_loss_avg + reg\n",
    "        # log_losses.append(log_loss_avg)\n",
    "        # grad_l2 = np.linalg.norm(grad.values)\n",
    "        # w_l2 = np.linalg.norm(w.values)\n",
    "        \n",
    "        # Regularization\n",
    "        w0 = w[0][0]     # Exclude w0\n",
    "        w -= (alpha * _lambda) * w\n",
    "        w[0][0] = w0\n",
    "        \n",
    "        y_pred_train = y_train - (sigmoid(X_train.dot(w)).rename(columns={\"Response\":0}) >= 0.5).astype(int)\n",
    "        acc = (y_pred_train == 0).sum()[0] / n\n",
    "        acc_train.append(acc)\n",
    "        \n",
    "        y_pred_val = y_val - (sigmoid(X_val.dot(w)).rename(columns={\"Response\":0}) >= 0.5).astype(int)\n",
    "        acc = (y_pred_val == 0).sum()[0] / len(X_val)\n",
    "        acc_val.append(acc)\n",
    "        \n",
    "        # Check for convergence\n",
    "        \n",
    "        magLw = np.linalg.norm(grad.values) / n # magnitude of gradient (change in weights)\n",
    "        grad_vals.append(magLw)\n",
    "        \n",
    "        if iter % 1000 == 0:\n",
    "            print(\"Iteration 1000, magLw = %f, change rate = %f\" % (magLw, abs(grad_vals[-1] - grad_vals[-2])))\n",
    "            \n",
    "        # Check for difference between cur and past gradient, if minimal change then model has converged\n",
    "        if (len(grad_vals) > 2) and (abs(grad_vals[-1] - grad_vals[-2]) < epsilon):\n",
    "            print(\"learning_rate = %f, lambda = %f, converged at iter #%d\" % (alpha, _lambda, iter))\n",
    "            converged = True # set model to converged, return weights and train/val acc\n",
    "            \n",
    "        iter += 1\n",
    "    \n",
    "    return w, acc_train, acc_val\n",
    "\n",
    "_lambdas = [10**(x) for x in range(-4,3)] # generate values of lambda 10^i, i in [-4,2]\n",
    "lrs = [0.05, 0.04, 0.03, 0.035, 0.08, 0.001, 0.0001, 0.000004]\n",
    "\n",
    "acc_train_lmb = {} # dictionary to store train acc at each iteration for all models, indexed by lambda\n",
    "acc_val_lmb = {} # dictionary to store val acc at each iteration for all models, indexed by lambda\n",
    "w_train_lmb = {} # dictionary to store weights for all models, indexed by learning rate\n",
    "\n",
    "for lmbd, lr in zip(_lambdas, lrs):\n",
    "    w,acc_train,acc_val = LR_L2_train(df_norm_train, df_norm_val, lmbd, lr, False) # train model with current lambda value\n",
    "    acc_train_lmb[str(lmbd)] = acc_train # store training accuracy, indexed by lambda value\n",
    "    acc_val_lmb[str(lmbd)] = acc_val # store val accuracy, indexed by lambda value\n",
    "    w_train_lmb[str(lmbd)] = w # store final training weights, indexed by lambda value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from IA2_skeleton_code import plot_losses,sparsity_graph\n",
    "\n",
    "plot_losses(acc_train_lmb, acc_val_lmb, \"L2_accuracy_per_iteration.jpg\")\n",
    "\n",
    "acc_train = []\n",
    "acc_val = []\n",
    "for i in acc_train_lmb.keys():\n",
    "    acc_train.append(acc_train_lmb[str(i)][-1])\n",
    "    acc_val.append(acc_val_lmb[str(i)][-1])\n",
    "\n",
    "plt.scatter(np.log10(list(acc_train_lmb.keys())), acc_train, label=\"Train accuracy\")\n",
    "plt.scatter(np.log10(list(acc_val_lmb.keys())), acc_val, label=\"Validation accuracy\")\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['axes.titlesize'] = 12\n",
    "plt.rcParams[\"legend.edgecolor\"] = 'black'\n",
    "plt.rcParams[\"legend.fontsize\"] = 10\n",
    "plt.legend()\n",
    "plt.savefig(\"L2_accuracy_per_lambda.jpg\")\n",
    "\n",
    "sparsity_graph(w_train_lmb, \"SparsityL2.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IA2_skeleton_code import LR_L2_train_torch\n",
    "\n",
    "_lambdas = [10**(x) for x in range(-4,3)] # generate values of lambda 10^i, i in [-4,2]\n",
    "lrs = [0.05, 0.04, 0.03, 0.035, 0.08, 0.001, 0.0001, 0.000004]\n",
    "\n",
    "acc_train_lmb = {} # dictionary to store train acc at each iteration for all models, indexed by lambda\n",
    "acc_val_lmb = {} # dictionary to store val acc at each iteration for all models, indexed by lambda\n",
    "w_train_lmb = {} # dictionary to store weights for all models, indexed by learning rate\n",
    "\n",
    "for lmbd, lr in zip(_lambdas, lrs):\n",
    "    w,acc_train,acc_val = LR_L2_train_torch(df_norm_train, df_norm_val, lmbd, lr, False) # train model with current lambda value\n",
    "    acc_train_lmb[str(lmbd)] = acc_train # store training accuracy, indexed by lambda value\n",
    "    acc_val_lmb[str(lmbd)] = acc_val # store val accuracy, indexed by lambda value\n",
    "    w_train_lmb[str(lmbd)] = w # store final training weights, indexed by lambda value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IA2_skeleton_code import plot_losses,sparsity_graph\n",
    "\n",
    "# plot_losses(acc_train_lmb, acc_val_lmb, \"L2_accuracy_per_iteration.jpg\")\n",
    "\n",
    "acc_train = []\n",
    "acc_val = []\n",
    "for i in acc_train_lmb.keys():\n",
    "    acc_train.append(acc_train_lmb[str(i)][-1])\n",
    "    acc_val.append(acc_val_lmb[str(i)][-1])\n",
    "\n",
    "log10_lambdas = [np.log10(float(x)) for x in acc_train_lmb.keys()]\n",
    "\n",
    "plt.scatter(log10_lambdas, acc_train, label=\"Train accuracy\")\n",
    "plt.scatter(log10_lambdas, acc_val, label=\"Validation accuracy\")\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['axes.titlesize'] = 12\n",
    "plt.rcParams[\"legend.edgecolor\"] = 'black'\n",
    "plt.rcParams[\"legend.fontsize\"] = 10\n",
    "plt.legend()\n",
    "plt.savefig(\"L2_accuracy_per_lambda.jpg\")\n",
    "\n",
    "sparsity_graph(w_train_lmb, \"SparsityL2.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lmbd_val = \"\"\n",
    "max_acc_val = 0\n",
    "for lmbda,acc in acc_val_lmb.items():\n",
    "    if acc[-1] > max_acc_val:\n",
    "        best_lmbd_val = lmbda\n",
    "        max_acc_val = acc[-1]\n",
    "        \n",
    "print(\"Best model has %f accuracy and lambda as %s\" % (max_acc_val, best_lmbd_val))\n",
    "\n",
    "best_lmbd_val = \"\"\n",
    "max_acc_train = 0\n",
    "for lmbda,acc in acc_train_lmb.items():\n",
    "    if acc[-1] > max_acc_train:\n",
    "        best_lmbd_val = lmbda\n",
    "        max_acc_train = acc[-1]\n",
    "        \n",
    "print(\"Best model has %f accuracy and lambda as %s\" % (max_acc_val, best_lmbd_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 5 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_w_train_lmb = w_train_lmb.copy()\n",
    "top_5_features = {}\n",
    "\n",
    "for lamb in _lambdas:\n",
    "    i = 0\n",
    "    top_5_features[str(lamb)] = {}\n",
    "    while i < 5:\n",
    "        idx_max = tmp_w_train_lmb[str(lamb)].idxmax()\n",
    "        top_5_features[str(lamb)][idx_max[0]] = float(tmp_w_train_lmb[str(lamb)].loc[idx_max[0]])\n",
    "        tmp_w_train_lmb[str(lamb)] = tmp_w_train_lmb[str(lamb)].drop(idx_max[0])\n",
    "        i += 1\n",
    "        # w_train_lmb[str(lamb)]\n",
    "\n",
    "for lamb in _lambdas:\n",
    "    print(\"Top 5 features of model trained by lambda = %s:\" % str(lamb))\n",
    "    print(top_5_features[str(lamb)])\n",
    "    print()\n",
    "\n",
    "# top_5_features\n",
    "w_train_lmb[\"0.0001\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IA2_skeleton_code import LR_L2_train\n",
    "\n",
    "df_train_noisy = pd.read_csv(\"IA2-train-noisy.csv\")\n",
    "\n",
    "df_norm_train_noisy = df_train_noisy\n",
    "df_norm_val_noisy = df_val.copy()\n",
    "\n",
    "for col in numerical_feas:\n",
    "    mu = df_norm_train_noisy[col].mean()\n",
    "    sigma = df_norm_train_noisy[col].std()\n",
    "\n",
    "    df_norm_train_noisy[col] = normalize(df_train_noisy, col, mu, sigma)\n",
    "    df_norm_val_noisy[col] = normalize(df_val, col, mu, sigma)\n",
    "\n",
    "acc_noisy_train_lmb = {} # dictionary to store train acc at each iteration for all models, indexed by lambda\n",
    "acc_noisy_val_lmb = {} # dictionary to store val acc at each iteration for all models, indexed by lambda\n",
    "w_noisy_train_lmb = {} # dictionary to store weights for all models, indexed by learning rate\n",
    "\n",
    "_lambdas = [10**(x) for x in range(-4,3)] # generate values of lambda 10^i, i in [-4,2]\n",
    "lrs = [0.05, 0.04, 0.03, 0.035, 0.08, 0.001, 0.0001, 0.000004]\n",
    "\n",
    "for lmbd, lr in zip(_lambdas, lrs):\n",
    "    w,acc_train,acc_val = LR_L2_train(df_norm_train_noisy, df_norm_val_noisy, lmbd, lr, True) # train model with current lambda value\n",
    "    acc_noisy_train_lmb[str(lmbd)] = acc_train # store training accuracy, indexed by lambda value\n",
    "    acc_noisy_val_lmb[str(lmbd)] = acc_val # store val accuracy, indexed by lambda value\n",
    "    w_noisy_train_lmb[str(lmbd)] = w # store final training weights, indexed by lambda value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IA2_skeleton_code import plot_losses,sparsity_graph\n",
    "\n",
    "plot_losses(acc_noisy_train_lmb, acc_noisy_val_lmb, \"L2_noisy_accuracy_per_iteration.jpg\")\n",
    "\n",
    "acc_train = []\n",
    "acc_val = []\n",
    "for i in _lambdas:\n",
    "    acc_train.append(acc_noisy_train_lmb[str(i)][-1])\n",
    "    acc_val.append(acc_noisy_val_lmb[str(i)][-1])\n",
    "\n",
    "fig2 = plt.figure(figsize=(8,6))\n",
    "\n",
    "plt.scatter(np.log10(_lambdas), acc_train, label=\"Train accuracy\")\n",
    "plt.scatter(np.log10(_lambdas), acc_val, label=\"Validation accuracy\")\n",
    "plt.legend()\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['axes.titlesize'] = 12\n",
    "plt.rcParams[\"legend.edgecolor\"] = 'black'\n",
    "plt.rcParams[\"legend.fontsize\"] = 10\n",
    "plt.savefig(\"L2_noisy_accuracy_per_lambda.jpg\")\n",
    "\n",
    "sparsity_graph(w_noisy_train_lmb, \"L2_noisy_sparsity.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART III"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IA2_skeleton_code import LR_L1_train\n",
    "\n",
    "_lambdas = [10**(x) for x in range(-4,3)] # generate values of lambda 10^i, i in [-4,2]\n",
    "lrs = [0.01, 0.03, 0.02, 0.08, 0.08, 0.5, 0.0001] # Learning rates that work best for corresponding lambda\n",
    "\n",
    "train_acc_lmb = {} # dictionary to store train acc at each iteration for all models, indexed by lambda\n",
    "val_acc_lmb = {} # dictionary to store val acc at each iteration for all models, indexed by lambda\n",
    "train_w_lmb = {} # dictionary to store weights for all models, indexed by learning rate\n",
    "\n",
    "for lmbd, lr in zip(_lambdas, lrs): # Iterate over lambdas and correlated learning rates\n",
    "    tmpW, tmpTA, tmpVA = LR_L1_train(df_norm_train, df_norm_val, lmbd, lr) # train model with current lambda value\n",
    "    train_acc_lmb[str(lmbd)] = tmpTA # store training accuracy, indexed by lambda value\n",
    "    val_acc_lmb[str(lmbd)] = tmpVA # store val accuracy, indexed by lambda value\n",
    "    train_w_lmb[str(lmbd)] = tmpW # store final training weights, indexed by lambda value\n",
    "    \n",
    "plot_losses(train_acc_lmb, val_acc_lmb, \"L1_Accuracy.jpg\") # plot accuracy train & val\n",
    "\n",
    "sparsity_graph(train_w_lmb, \"SparsityL1.jpg\") # plot sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train = []\n",
    "acc_val = []\n",
    "for i in _lambdas:\n",
    "    acc_train.append(acc_train_lmb[str(i)][-1])\n",
    "    acc_val.append(acc_val_lmb[str(i)][-1])\n",
    "\n",
    "plt.scatter(np.log10(_lambdas), acc_train, label=\"Train accuracy\")\n",
    "plt.scatter(np.log10(_lambdas), acc_val, label=\"Validation accuracy\")\n",
    "plt.legend()\n",
    "plt.savefig(\"L1_accuracy_per_lambda.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train_w_lmb['1'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "15a1e72ba773099527125ed58d42668c54ab9c98f183bb59c28e4748fcdd480d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
